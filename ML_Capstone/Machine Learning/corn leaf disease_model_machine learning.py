# -*- coding: utf-8 -*-
"""Capstone3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RW2VlL5ybVT8DmHMv-ieJNTQBvGrVgd-
"""

import zipfile
import numpy as np
import pandas as pd
import os
from PIL import Image
from collections import defaultdict
import shutil
import matplotlib.pyplot as plt

"""# Data Loading

dataset penyakit daun jagung bersumber dari kaggle (https://www.kaggle.com/datasets/abdelrahmanemad2199/corn-or-maize-leaf-disease-dataset). untuk memudahkan data load, saya upload dataset ke google drive
"""

!gdown 1KmGregqrpebOk77fM6MonWHGpLRr4YGQ

"""#### extract file zip dataset"""

# extract folder zip

with zipfile.ZipFile('/content/archive (41).zip', 'r') as zip_ref:
    zip_ref.extractall('dataset')

"""#### memeriksa jumlah kelas dalam dataset dan menampilkan resolusi gambar di dalam dataset"""

# Fungsi untuk mengecek folder dan resolusi gambar
def check_folders_and_resolutions(directory):
    # Menyimpan nama folder, resolusi gambar, dan jumlah gambar di setiap folder
    folder_list = []
    image_resolutions = []
    folder_image_count = {}
    resolution_count = defaultdict(int)  # Menyimpan jumlah gambar berdasarkan resolusi

    # Menjelajah direktori utama dan subfolder
    for root, dirs, files in os.walk(directory):
        # Menyimpan subfolder di dalam folder_list
        for dir_name in dirs:
            folder_list.append(os.path.join(root, dir_name))  # Menambahkan path lengkap subfolder
            folder_image_count[os.path.join(root, dir_name)] = 0  # Inisialisasi jumlah gambar di folder

        # Mengecek file gambar di dalam folder (termasuk subfolder)
        for file_name in files:
            file_path = os.path.join(root, file_name)

            # Mengecek apakah file merupakan gambar
            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
                try:
                    with Image.open(file_path) as img:
                        # Dapatkan resolusi gambar
                        resolution = img.size  # Resolusi berupa tuple (lebar, tinggi)
                        image_resolutions.append((file_name, resolution))

                        # Update jumlah gambar di folder
                        folder_image_count[root] += 1

                        # Mengelompokkan resolusi dan menghitung jumlah gambar untuk masing-masing resolusi
                        resolution_count[resolution] += 1
                except Exception as e:
                    print(f"Error membuka {file_name}: {e}")

    # Menampilkan hasil folder yang ditemukan dan jumlah gambar di masing-masing folder
    print("Folder yang ditemukan di dalam direktori beserta jumlah gambar:")
    for folder in folder_list:
        print(f"- {folder}: {folder_image_count[folder]} gambar")

    # Menampilkan jumlah gambar berdasarkan resolusi
    print("\nJumlah gambar berdasarkan resolusi:")
    for resolution, count in resolution_count.items():
        print(f"- {resolution[0]}x{resolution[1]}: {count} gambar")

# Tentukan path folder yang ingin Anda cek
directory = '/content/dataset/corn/data'  # Gantilah dengan path folder Anda

# Panggil fungsi untuk mengecek folder dan resolusi gambar
check_folders_and_resolutions(directory)

import os
import pandas as pd

sdir = '/content/dataset/archive (41)/corn/data'

# Inisialisasi list untuk menyimpan file paths dan labels
filepaths = []
labels = []

# Menyusun list kelas berdasarkan subfolder dalam direktori
classlist = os.listdir(sdir)

# Menyusun file paths dan labels untuk setiap file
for klass in classlist:
    classpath = os.path.join(sdir, klass)
    if os.path.isdir(classpath):  # Memeriksa apakah itu direktori
        flist = os.listdir(classpath)
        for f in flist:
            fpath = os.path.join(classpath, f)
            filepaths.append(fpath)
            labels.append(klass)

# Membuat DataFrame dari file paths dan labels
df = pd.DataFrame({
    'image_path': filepaths,
    'category': labels  # Label berdasarkan nama subfolder
})

# Menampilkan DataFrame untuk memeriksa hasil
print(df.head())

# Menampilkan jumlah label per kategori
print(df['category'].value_counts())

"""#### menampilkan gambar masing-masing dataset daun jagung"""

# Path to the image directory
image_dir = '/content/dataset/archive (41)/corn/data'

# Get a list of subfolders (classes)
class_folders = [f for f in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, f))]

# Iterate through each class folder
for class_folder in class_folders:
  class_path = os.path.join(image_dir, class_folder)
  images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg'))]

  # Display up to 5 images per class
  num_images_to_display = min(5, len(images))
  fig, axes = plt.subplots(1, num_images_to_display, figsize=(15, 3))
  fig.suptitle(class_folder, fontsize=16)

  for i in range(num_images_to_display):
      image_path = os.path.join(class_path, images[i])
      try:
          img = plt.imread(image_path)
          axes[i].imshow(img)
          axes[i].axis('off') # Hide axes
      except Exception as e:
          print(f"Error displaying image {image_path}: {e}")
          axes[i].axis('off')

  plt.show()

"""# Data Preprocessing"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Fungsi utama untuk training dan validasi
def create_data_generators(train_df, validation_df, target_size=(224, 224), batch_size=64):
    # Augmentasi + normalisasi untuk training
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest'
    )

    # Hanya rescale untuk validasi
    valid_datagen = ImageDataGenerator(rescale=1./255)

    # Generator training
    train_generator = train_datagen.flow_from_dataframe(
        dataframe=train_df,
        x_col='image_path',
        y_col='category',
        target_size=target_size,
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=True  # penting untuk training
    )

    # Generator validasi
    valid_generator = valid_datagen.flow_from_dataframe(
        dataframe=validation_df,
        x_col='image_path',
        y_col='category',
        target_size=target_size,
        batch_size=batch_size,
        class_mode='categorical',
        shuffle=False  # biasanya tidak diacak untuk evaluasi
    )

    return train_generator, valid_generator

# Fungsi tambahan untuk test set (rescale only)
def create_test_generator(test_df, target_size=(224, 224), batch_size=64):
    test_datagen = ImageDataGenerator(rescale=1./255)
    test_generator = test_datagen.flow_from_dataframe(
        dataframe=test_df,
        x_col='image_path',
        y_col=None,  # biasanya tidak ada label
        target_size=target_size,
        batch_size=batch_size,
        class_mode=None,
        shuffle=False
    )
    return test_generator

# Misalnya, resized_images_df adalah DataFrame yang sudah berisi gambar yang telah diresize dan informasinya.
# Mari kita bagi data menjadi 80% untuk pelatihan dan 20% untuk validasi dan pengujian.

from sklearn.model_selection import train_test_split # Import train_test_split

# Pembagian data pertama, 80% untuk pelatihan dan 20% untuk validasi dan pengujian
df, remaining_df = train_test_split(df, train_size=0.8, random_state=123, shuffle=True)  # 80% untuk pelatihan

# Pembagian data kedua, 10% untuk validasi dan 10% untuk pengujian (dari remaining_df yang berisi 20%)
valid_df, test_df = train_test_split(remaining_df, test_size=0.5, random_state=123, shuffle=True)  # 50% dari 20% untuk validasi dan 50% untuk pengujian

# Menampilkan panjang setiap set data untuk memverifikasi pembagian
print(f'train_df length: {len(df)}')
print(f'valid_df length: {len(valid_df)}')
print(f'test_df length: {len(test_df)}')

from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Import necessary modules for model creation
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam

def create_model(input_shape=(224, 224, 3), num_classes=10):
    # Memuat MobileNetV2 tanpa bagian fully connected layer (include_top=False)
    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)

    # Membekukan semua layer dari MobileNetV2 agar tidak dilatih ulang
    for layer in base_model.layers:
        layer.trainable = False

    # Membangun model dengan menambahkan beberapa layer Conv2D, MaxPooling2D, dan Dense
    model = models.Sequential()
    model.add(base_model)  # Menambahkan MobileNetV2 sebagai base model

    # Menambahkan beberapa layer Conv2D dan MaxPooling2D untuk ekstraksi fitur lebih lanjut
    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2)))
    model.add(layers.Flatten())

    # Menambahkan layer Dense untuk klasifikasi
    model.add(layers.Dense(512, activation='relu'))
    model.add(layers.Dense(num_classes, activation='softmax'))  # Output layer dengan softmax untuk klasifikasi multikelas

    # Kompilasi model dengan optimasi Adam dan loss categorical_crossentropy
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Misalnya, kita ingin memiliki 10 kelas
num_classes = 4
model = create_model(num_classes=num_classes)

# Menampilkan ringkasan model
model.summary()

from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Import necessary modules for model creation
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam
# Import necessary callbacks
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

# Callback EarlyStopping dengan penyesuaian
early_stopping = EarlyStopping(
    monitor='val_loss',            # Monitor val_loss untuk menghentikan pelatihan lebih cepat jika loss tidak membaik
    patience=3,                    # Menunggu 3 epoch tanpa peningkatan
    min_delta=0.001,               # Perubahan minimal untuk dianggap perbaikan
    restore_best_weights=True,     # Mengembalikan bobot model terbaik
    mode='min',                    # Maksimalkan perbaikan val_loss (semakin kecil semakin baik)
    baseline=0.96                  # Baseline untuk akurasi, bisa diganti dengan 0.96 (lebih ke performance metric lainnya)
)

# Callback ModelCheckpoint untuk menyimpan model terbaik
model_checkpoint = ModelCheckpoint(
    'best_model.keras',
    monitor='val_loss',            # Memantau val_loss
    save_best_only=True,           # Hanya menyimpan model dengan val_loss terbaik
    mode='min',                    # Semakin kecil loss, semakin baik
    verbose=1                      # Tampilkan info saat model disimpan
)

# Learning rate scheduler untuk mengurangi learning rate jika val_loss tidak menurun
lr_scheduler = ReduceLROnPlateau(
    monitor='val_loss',           # Memantau val_loss
    factor=0.2,                   # Mengurangi learning rate dengan faktor 0.2
    patience=2,                   # Setelah 2 epoch, kurangi learning rate jika tidak ada perbaikan
    verbose=1,                    # Menampilkan informasi perubahan learning rate
    min_lr=1e-6                   # Batas bawah untuk learning rate
)

# Gabungkan callback menjadi satu list
callbacks = [early_stopping, model_checkpoint, lr_scheduler]

train_generator, valid_generator = create_data_generators(df, valid_df)

# Melatih model dengan menggunakan callbacks yang telah dibuat
history = model.fit(
    train_generator,  # Generator untuk data pelatihan
    epochs=10,        # Jumlah epoch pelatihan
    validation_data=valid_generator,  # Generator untuk data validasi
    callbacks=callbacks,  # Menggunakan callbacks yang telah Anda buat sebelumnya
    verbose=1          # Menampilkan progres pelatihan di konsol
)

# ... (The rest of your code) ...

import matplotlib.pyplot as plt

# Evaluasi model menggunakan data validasi
loss, accuracy = model.evaluate(valid_generator, verbose=1)
print(f"Loss pada data validasi: {loss}")
print(f"Akurasi pada data validasi: {accuracy}")

# Mendapatkan riwayat pelatihan dari objek history
train_acc = history.history['accuracy']
valid_acc = history.history['val_accuracy']
train_loss = history.history['loss']
valid_loss = history.history['val_loss']

# Plot grafik akurasi
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)  # Grafik pertama untuk akurasi
plt.plot(train_acc, label='Akurasi Pelatihan')
plt.plot(valid_acc, label='Akurasi Validasi')
plt.title('Grafik Akurasi Pelatihan dan Validasi')
plt.xlabel('Epoch')
plt.ylabel('Akurasi')
plt.legend()

# Plot grafik loss
plt.subplot(1, 2, 2)  # Grafik kedua untuk loss
plt.plot(train_loss, label='Loss Pelatihan')
plt.plot(valid_loss, label='Loss Validasi')
plt.title('Grafik Loss Pelatihan dan Validasi')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Menampilkan kedua grafik
plt.tight_layout()
plt.show()

# prompt: buatkan evalusi lanjutan dengan menampilkan presisi, recal, f1 score dll

import numpy as np
from sklearn.metrics import classification_report

# Create the test generator
test_generator = create_test_generator(test_df)

# Make predictions on the test data
predictions = model.predict(test_generator)
predicted_classes = np.argmax(predictions, axis=1)

# Get true labels from the test DataFrame
# true_classes = test_generator.classes  # Incorrect: DataFrameIterator has no 'classes'
true_classes = test_df['category'].astype('category').cat.codes # Correct: Access labels from the DataFrame

# Get class labels from the test generator
# class_labels = list(test_generator.class_indices.keys()) # Incorrect: DataFrameIterator used for prediction has no class_indices
class_labels = list(df['category'].unique()) # Correct: Getting unique class labels from training data

# Generate the classification report
report = classification_report(true_classes, predicted_classes, target_names=class_labels)

# Print the report
print(report)

model.save("model.h5")

!pip install tensorflowjs

# Mengonversi model Keras (.h5) ke format TensorFlow.js (.json)
!tensorflowjs_converter --input_format=keras model.h5 tfjs_model

# Python untuk membuat label.txt
import os

# Tentukan path ke dataset yang berisi subfolder untuk setiap kelas
dataset_dir = '/content/dataset/archive (41)/corn/data'  # Ganti dengan path dataset Anda

# Ambil daftar subfolder yang mewakili kelas
class_labels = sorted(os.listdir(dataset_dir))

# Tulis nama-nama kelas ke file label.txt
with open('label.txt', 'w') as f:
    for label in class_labels:
        f.write(f"{label}\n")

print("File label.txt telah dibuat!")

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import os

# Fungsi untuk memuat model TensorFlow (menggunakan load_model untuk .h5)
def load_keras_model(model_path):
    # Menggunakan tf.keras.models.load_model untuk memuat model .h5
    model = tf.keras.models.load_model(model_path)
    return model

# Fungsi untuk memproses gambar agar sesuai dengan input model
def preprocess_image(image_path, target_size=(224, 224)):
    img = Image.open(image_path).convert('RGB')  # Pastikan 3 channel
    img = img.resize(target_size)
    img_array = np.array(img)
    img_array = np.expand_dims(img_array, axis=0)  # Tambahkan dimensi batch
    img_array = img_array / 255.0  # Normalisasi piksel
    return img_array

# Fungsi untuk melakukan inferensi dan menampilkan hasil
def predict_and_display(image_path, model, class_labels):
    try:
        # Proses gambar
        processed_image = preprocess_image(image_path)

        # Lakukan prediksi
        # Untuk model Keras yang dimuat dengan load_model, gunakan model.predict
        predictions = model.predict(processed_image)
        predicted_class_index = np.argmax(predictions, axis=1)[0]
        predicted_class = class_labels[predicted_class_index]
        confidence = predictions[0][predicted_class_index] * 100

        # Tampilkan gambar dan prediksi
        img = Image.open(image_path)
        plt.imshow(img)
        plt.title(f"Predicted: {predicted_class} ({confidence:.2f}%)")
        plt.axis('off')
        plt.show()

    except FileNotFoundError:
        print(f"Error: Gambar tidak ditemukan di {image_path}")
    except Exception as e:
        print(f"Terjadi kesalahan saat prediksi atau menampilkan gambar: {e}")

# Main program
def main():
    # Path ke model Keras .h5
    saved_model_path = '/content/model.h5'  # Path model .h5
    label_file_path = '/content/label.txt'  # Path file label Anda

    # Muat model menggunakan fungsi yang diperbarui
    model = load_keras_model(saved_model_path)

    # Muat label dari file
    with open(label_file_path, 'r') as f:
        class_labels = f.read().splitlines()

    # Ambil gambar dari lokal untuk prediksi
    image_path = input("Masukkan path gambar yang ingin diprediksi: ")

    if not os.path.exists(image_path):
        print("Gambar tidak ditemukan di path yang diberikan!")
        return

    # Lakukan prediksi dan tampilkan gambar
    predict_and_display(image_path, model, class_labels)

if __name__ == "__main__":
    main()